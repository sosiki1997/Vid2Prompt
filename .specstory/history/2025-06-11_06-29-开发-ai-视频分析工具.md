<!-- Generated by SpecStory -->

# 开发 AI 视频分析工具 (2025-06-11 15:29:49)

_**User**_

我要开发一个工具，实现以下目标：

用户上传一个 AI 生成的视频后，程序可以：
1. 自动抽取视频中 3~5 帧图像（等间隔）
2. 使用 CLIP 模型（或 clip-interrogator）生成每一帧的文本描述（prompt）
3. 汇总这些描述，生成一个综合性的推测 prompt
4. 根据图像风格关键词和 prompt，推测可能使用的生成模型（如 Sora、Runway、AnimateDiff 等）
5. 使用 Streamlit 实现一个界面，展示视频、关键帧、推测 prompt、猜测平台

我打算使用 Python 实现，先从视频帧提取和 CLIP 分析开始。


---

_**Assistant**_

我将帮您开发这个工具。让我们从检查项目结构开始，然后逐步实现功能。

---

<details>
            <summary>Listed current directory • **2** results</summary>
        
| Name |
|-------|
| 📁 `.specstory` |
| 📄 `.cursorindexingignore` |

</details>

---

我们需要从头开始创建项目。首先创建必要的文件结构和依赖文件。

---

```diff
- 
+ streamlit==1.32.0
+ opencv-python==4.8.1.78
+ pillow==10.1.0
+ numpy==1.26.2
+ torch==2.1.2
+ torchvision==0.16.2
+ clip-interrogator==0.6.0
+ open-clip-torch==2.23.0
+ transformers==4.36.2
+ moviepy==1.0.3 
```

---

现在创建主应用文件：

---

```diff
- 
+ import streamlit as st
+ import os
+ import tempfile
+ from PIL import Image
+ import numpy as np
+ import torch
+ 
+ from src.video_processor import extract_frames
+ from src.prompt_generator import analyze_frames, generate_combined_prompt
+ from src.model_detector import guess_generation_model
+ 
+ st.set_page_config(page_title="视频Prompt分析器", layout="wide")
+ 
+ st.title("AI生成视频Prompt分析器")
+ st.write("上传AI生成的视频，分析可能使用的提示词和生成模型")
+ 
+ uploaded_file = st.file_uploader("上传视频文件", type=["mp4", "avi", "mov", "mkv"])
+ 
+ if uploaded_file is not None:
+     # 保存上传的视频到临时文件
+     with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
+         tmp_file.write(uploaded_file.getvalue())
+         video_path = tmp_file.name
+     
+     # 显示视频
+     st.video(video_path)
+     
+     with st.spinner('正在处理视频...'):
+         # 提取帧
+         frames, timestamps = extract_frames(video_path, num_frames=5)
+         
+         # 分析每一帧
+         frame_prompts = analyze_frames(frames)
+         
+         # 生成综合提示词
+         combined_prompt = generate_combined_prompt(frame_prompts)
+         
+         # 猜测生成模型
+         model_guess = guess_generation_model(frames, combined_prompt)
+     
+     # 显示结果
+     st.header("分析结果")
+     
+     # 显示关键帧
+     st.subheader("关键帧")
+     cols = st.columns(len(frames))
+     for i, (frame, timestamp, prompt) in enumerate(zip(frames, timestamps, frame_prompts)):
+         with cols[i]:
+             st.image(frame, caption=f"时间: {timestamp:.2f}秒")
+             with st.expander("查看描述"):
+                 st.write(prompt)
+     
+     # 显示推测的提示词
+     st.subheader("推测的提示词")
+     st.text_area("综合提示词", combined_prompt, height=150)
+     
+     # 显示猜测的生成模型
+     st.subheader("可能使用的生成模型")
+     st.info(f"推测模型: {model_guess['model']}")
+     st.write(f"置信度: {model_guess['confidence']:.2f}")
+     st.write("模型特征:")
+     for feature, score in model_guess['features'].items():
+         st.write(f"- {feature}: {score:.2f}")
+     
+     # 清理临时文件
+     os.unlink(video_path) 
```

---

现在让我们创建项目的目录结构：

---

```bash
mkdir -p src
```

```
orange@orangedeMacBook-Air Vid2Prompt % 

```

---

接下来，我们创建视频处理模块：

---

```diff
- 
+ import cv2
+ import numpy as np
+ from typing import List, Tuple
+ import moviepy.editor as mp
+ 
+ def extract_frames(video_path: str, num_frames: int = 5) -> Tuple[List[np.ndarray], List[float]]:
+     """
+     从视频中等间隔提取指定数量的帧
+     
+     参数:
+         video_path: 视频文件路径
+         num_frames: 要提取的帧数量
+         
+     返回:
+         frames: 提取的帧列表
+         timestamps: 每帧对应的时间戳（秒）
+     """
+     # 打开视频文件
+     cap = cv2.VideoCapture(video_path)
+     
+     # 获取视频信息
+     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+     fps = cap.get(cv2.CAP_PROP_FPS)
+     duration = total_frames / fps
+     
+     # 计算采样间隔
+     if num_frames > total_frames:
+         num_frames = total_frames
+     
+     if num_frames <= 1:
+         frame_indices = [0]
+     else:
+         # 确保第一帧和最后一帧被包含，其余帧等间隔分布
+         frame_indices = [int(i * (total_frames - 1) / (num_frames - 1)) for i in range(num_frames)]
+     
+     # 提取帧
+     frames = []
+     timestamps = []
+     
+     for idx in frame_indices:
+         # 设置帧位置
+         cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
+         ret, frame = cap.read()
+         
+         if ret:
+             # 转换BGR到RGB
+             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
+             frames.append(frame_rgb)
+             
+             # 计算时间戳
+             timestamp = idx / fps
+             timestamps.append(timestamp)
+     
+     # 释放视频
+     cap.release()
+     
+     return frames, timestamps
+ 
+ def get_video_info(video_path: str) -> dict:
+     """
+     获取视频的基本信息
+     
+     参数:
+         video_path: 视频文件路径
+         
+     返回:
+         包含视频信息的字典
+     """
+     cap = cv2.VideoCapture(video_path)
+     
+     # 获取视频基本信息
+     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+     fps = cap.get(cv2.CAP_PROP_FPS)
+     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+     duration = frame_count / fps
+     
+     # 获取视频编解码器信息
+     fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))
+     fourcc_str = "".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])
+     
+     # 尝试获取音频信息
+     try:
+         video = mp.VideoFileClip(video_path)
+         has_audio = video.audio is not None
+         audio_fps = video.audio.fps if has_audio else None
+         video.close()
+     except:
+         has_audio = False
+         audio_fps = None
+     
+     cap.release()
+     
+     return {
+         "width": width,
+         "height": height,
+         "fps": fps,
+         "frame_count": frame_count,
+         "duration": duration,
+         "codec": fourcc_str,
+         "has_audio": has_audio,
+         "audio_fps": audio_fps
+     } 
```

---

现在创建提示词生成模块，使用CLIP-Interrogator来分析图像：

---

```diff
- 
+ import numpy as np
+ from PIL import Image
+ from typing import List, Dict, Any
+ import torch
+ from clip_interrogator import Config, Interrogator
+ import re
+ 
+ # 初始化CLIP Interrogator
+ def get_interrogator():
+     """获取CLIP Interrogator实例，使用单例模式避免重复加载"""
+     if not hasattr(get_interrogator, "instance"):
+         config = Config()
+         config.clip_model_name = "ViT-L-14/openai"  # 使用OpenAI的CLIP模型
+         config.chunk_size = 2048  # 减小此值可以减少内存使用
+         config.blip_num_beams = 8  # 减小此值可以减少内存使用
+         config.blip_max_length = 64  # 生成的描述最大长度
+         
+         # 检查是否有GPU可用
+         if torch.cuda.is_available():
+             config.device = "cuda"
+         else:
+             config.device = "cpu"
+         
+         # 创建Interrogator实例
+         get_interrogator.instance = Interrogator(config)
+     
+     return get_interrogator.instance
+ 
+ def analyze_frame(frame: np.ndarray) -> str:
+     """
+     分析单帧图像，生成描述
+     
+     参数:
+         frame: 图像数据，RGB格式的numpy数组
+         
+     返回:
+         生成的描述文本
+     """
+     # 转换numpy数组为PIL图像
+     image = Image.fromarray(frame)
+     
+     # 获取Interrogator实例
+     ci = get_interrogator()
+     
+     # 分析图像
+     try:
+         # 使用CLIP Interrogator生成描述
+         prompt = ci.interrogate(image)
+         return prompt
+     except Exception as e:
+         print(f"分析图像时出错: {str(e)}")
+         return "无法分析图像"
+ 
+ def analyze_frames(frames: List[np.ndarray]) -> List[str]:
+     """
+     分析多个视频帧，生成描述列表
+     
+     参数:
+         frames: 图像帧列表
+         
+     返回:
+         每帧对应的描述列表
+     """
+     prompts = []
+     for frame in frames:
+         prompt = analyze_frame(frame)
+         prompts.append(prompt)
+     return prompts
+ 
+ def extract_keywords(prompts: List[str]) -> Dict[str, int]:
+     """
+     从多个提示词中提取关键词及其频率
+     
+     参数:
+         prompts: 提示词列表
+         
+     返回:
+         关键词及其出现频率的字典
+     """
+     # 合并所有提示词
+     all_text = " ".join(prompts).lower()
+     
+     # 移除常见的修饰词和连接词
+     stop_words = {
+         "a", "an", "the", "and", "or", "but", "in", "on", "at", "to", "for", "with", 
+         "by", "of", "from", "as", "is", "are", "was", "were", "be", "been", "being",
+         "have", "has", "had", "do", "does", "did", "will", "would", "shall", "should",
+         "may", "might", "must", "can", "could"
+     }
+     
+     # 提取所有单词
+     words = re.findall(r'\b\w+\b', all_text)
+     
+     # 统计词频
+     word_freq = {}
+     for word in words:
+         if word not in stop_words and len(word) > 2:  # 忽略停用词和过短的词
+             word_freq[word] = word_freq.get(word, 0) + 1
+     
+     return word_freq
+ 
+ def generate_combined_prompt(prompts: List[str]) -> str:
+     """
+     根据多个帧的提示词生成一个综合提示词
+     
+     参数:
+         prompts: 每帧对应的提示词列表
+         
+     返回:
+         综合后的提示词
+     """
+     if not prompts:
+         return ""
+     
+     # 提取关键词及频率
+     keywords = extract_keywords(prompts)
+     
+     # 按频率排序关键词
+     sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
+     
+     # 提取风格关键词（通常在CLIP-Interrogator输出的末尾）
+     style_words = []
+     for prompt in prompts:
+         # CLIP-Interrogator通常会在描述后添加艺术风格关键词
+         parts = prompt.split(", ")
+         if len(parts) > 3:
+             # 取最后几个部分作为风格词
+             style_words.extend(parts[-3:])
+     
+     # 统计风格词频率
+     style_freq = {}
+     for word in style_words:
+         style_freq[word] = style_freq.get(word, 0) + 1
+     
+     # 按频率排序风格词
+     sorted_styles = sorted(style_freq.items(), key=lambda x: x[1], reverse=True)
+     
+     # 构建综合提示词
+     # 1. 从第一帧获取基本场景描述
+     base_description = prompts[0].split(",")[0]
+     
+     # 2. 添加出现频率最高的关键词（最多15个）
+     top_keywords = [word for word, freq in sorted_keywords[:15]]
+     
+     # 3. 添加最常见的风格词（最多5个）
+     top_styles = [style for style, freq in sorted_styles[:5]]
+     
+     # 组合最终提示词
+     combined = f"{base_description}, {', '.join(top_keywords)}, {', '.join(top_styles)}"
+     
+     return combined 
```

---

现在创建模型检测器模块，用于推测生成视频的AI模型：

---

```diff
- 
+ import numpy as np
+ from typing import List, Dict, Any
+ import re
+ 
+ # 定义不同AI生成模型的特征
+ MODEL_FEATURES = {
+     "Sora": {
+         "description": "OpenAI的视频生成模型",
+         "features": {
+             "高真实感": 0.9,
+             "复杂运动": 0.9,
+             "长时间连贯性": 0.85,
+             "物理准确性": 0.8,
+             "高分辨率": 0.8
+         },
+         "keywords": ["photorealistic", "realistic", "cinematic", "detailed", "4k", "8k"]
+     },
+     "Runway Gen-2": {
+         "description": "Runway的视频生成模型",
+         "features": {
+             "艺术风格": 0.8,
+             "创意转场": 0.7,
+             "风格多样性": 0.85,
+             "长时间连贯性": 0.6,
+             "分辨率": 0.7
+         },
+         "keywords": ["stylized", "artistic", "creative", "transition", "animation"]
+     },
+     "Pika Labs": {
+         "description": "Pika Labs的视频生成模型",
+         "features": {
+             "动画风格": 0.8,
+             "卡通效果": 0.75,
+             "创意表现": 0.8,
+             "短视频": 0.7,
+             "风格一致性": 0.75
+         },
+         "keywords": ["animation", "cartoon", "stylized", "creative", "character"]
+     },
+     "AnimateDiff": {
+         "description": "基于Stable Diffusion的动画生成模型",
+         "features": {
+             "动画风格": 0.9,
+             "二次元": 0.85,
+             "角色动画": 0.8,
+             "短循环": 0.9,
+             "风格一致性": 0.7
+         },
+         "keywords": ["anime", "animation", "character", "loop", "2d", "cartoon"]
+     },
+     "Stable Video Diffusion": {
+         "description": "Stability AI的视频扩散模型",
+         "features": {
+             "图像到视频": 0.9,
+             "短视频": 0.8,
+             "风格一致性": 0.75,
+             "有限运动": 0.7,
+             "分辨率": 0.65
+         },
+         "keywords": ["short", "consistent", "image-to-video", "diffusion"]
+     }
+ }
+ 
+ def analyze_video_features(frames: List[np.ndarray]) -> Dict[str, float]:
+     """
+     分析视频的特征
+     
+     参数:
+         frames: 视频帧列表
+         
+     返回:
+         视频特征评分字典
+     """
+     features = {}
+     
+     # 分析分辨率
+     if frames:
+         height, width = frames[0].shape[:2]
+         if width >= 1920 or height >= 1080:
+             features["高分辨率"] = 0.8
+         elif width >= 1280 or height >= 720:
+             features["高分辨率"] = 0.6
+         else:
+             features["高分辨率"] = 0.4
+     
+     # 分析帧间一致性
+     if len(frames) > 1:
+         frame_diffs = []
+         for i in range(len(frames) - 1):
+             # 计算相邻帧的差异
+             diff = np.mean(np.abs(frames[i+1].astype(float) - frames[i].astype(float)))
+             frame_diffs.append(diff)
+         
+         avg_diff = np.mean(frame_diffs)
+         if avg_diff < 20:  # 低差异表示高一致性
+             features["风格一致性"] = 0.9
+         elif avg_diff < 40:
+             features["风格一致性"] = 0.7
+         else:
+             features["风格一致性"] = 0.5
+     
+     # 分析色彩丰富度
+     if frames:
+         color_variance = np.mean([np.var(frame.reshape(-1, 3), axis=0).mean() for frame in frames])
+         if color_variance > 2000:
+             features["色彩丰富度"] = 0.8
+         elif color_variance > 1000:
+             features["色彩丰富度"] = 0.6
+         else:
+             features["色彩丰富度"] = 0.4
+     
+     return features
+ 
+ def analyze_prompt_features(prompt: str) -> Dict[str, float]:
+     """
+     分析提示词中的特征
+     
+     参数:
+         prompt: 提示词
+         
+     返回:
+         提示词特征评分字典
+     """
+     features = {}
+     prompt_lower = prompt.lower()
+     
+     # 检查是否包含真实感相关词汇
+     realism_keywords = ["realistic", "photorealistic", "real", "photograph", "cinematic", "4k", "8k", "hdr"]
+     realism_score = sum(1 for kw in realism_keywords if kw in prompt_lower) / len(realism_keywords)
+     if realism_score > 0:
+         features["高真实感"] = min(realism_score * 1.5, 1.0)
+     
+     # 检查是否包含动画相关词汇
+     animation_keywords = ["anime", "animation", "cartoon", "animated", "stylized", "2d", "character"]
+     animation_score = sum(1 for kw in animation_keywords if kw in prompt_lower) / len(animation_keywords)
+     if animation_score > 0:
+         features["动画风格"] = min(animation_score * 1.5, 1.0)
+     
+     # 检查是否包含艺术风格词汇
+     art_keywords = ["painting", "artwork", "illustration", "digital art", "concept art", "artistic"]
+     art_score = sum(1 for kw in art_keywords if kw in prompt_lower) / len(art_keywords)
+     if art_score > 0:
+         features["艺术风格"] = min(art_score * 1.5, 1.0)
+     
+     return features
+ 
+ def calculate_model_match(video_features: Dict[str, float], prompt_features: Dict[str, float], 
+                          prompt: str) -> Dict[str, Dict[str, Any]]:
+     """
+     计算视频与各个模型的匹配度
+     
+     参数:
+         video_features: 视频特征
+         prompt_features: 提示词特征
+         prompt: 原始提示词
+         
+     返回:
+         各模型的匹配分数及详情
+     """
+     results = {}
+     prompt_lower = prompt.lower()
+     
+     # 合并视频和提示词特征
+     combined_features = {**video_features, **prompt_features}
+     
+     for model_name, model_info in MODEL_FEATURES.items():
+         score = 0
+         total_weight = 0
+         matching_features = {}
+         
+         # 计算特征匹配度
+         for feature, model_score in model_info["features"].items():
+             if feature in combined_features:
+                 feature_score = combined_features[feature]
+                 # 特征匹配度 = 特征存在度 * 模型特征重要性
+                 match_score = feature_score * model_score
+                 score += match_score
+                 total_weight += model_score
+                 matching_features[feature] = feature_score
+         
+         # 检查关键词匹配
+         keyword_matches = []
+         for keyword in model_info["keywords"]:
+             if keyword in prompt_lower:
+                 keyword_matches.append(keyword)
+         
+         # 关键词匹配加分
+         keyword_score = len(keyword_matches) / len(model_info["keywords"]) if model_info["keywords"] else 0
+         score += keyword_score * 0.3  # 关键词匹配占30%的权重
+         total_weight += 0.3
+         
+         # 归一化分数
+         if total_weight > 0:
+             final_score = score / total_weight
+         else:
+             final_score = 0
+         
+         results[model_name] = {
+             "score": final_score,
+             "matching_features": matching_features,
+             "keyword_matches": keyword_matches
+         }
+     
+     return results
+ 
+ def guess_generation_model(frames: List[np.ndarray], prompt: str) -> Dict[str, Any]:
+     """
+     根据视频帧和提示词推测可能使用的生成模型
+     
+     参数:
+         frames: 视频帧列表
+         prompt: 推测的提示词
+         
+     返回:
+         包含推测结果的字典
+     """
+     # 分析视频特征
+     video_features = analyze_video_features(frames)
+     
+     # 分析提示词特征
+     prompt_features = analyze_prompt_features(prompt)
+     
+     # 计算各模型匹配度
+     model_matches = calculate_model_match(video_features, prompt_features, prompt)
+     
+     # 找出最匹配的模型
+     best_model = max(model_matches.items(), key=lambda x: x[1]["score"])
+     model_name = best_model[0]
+     match_info = best_model[1]
+     
+     # 构建结果
+     result = {
+         "model": model_name,
+         "confidence": match_info["score"],
+         "description": MODEL_FEATURES[model_name]["description"],
+         "features": match_info["matching_features"],
+         "keyword_matches": match_info["keyword_matches"]
+     }
+     
+     return result 
```

---

最后，让我们创建一个初始化文件和README：

---

```diff
- 
+ # Vid2Prompt - 视频提示词分析工具
+ # 用于从AI生成视频中提取可能的提示词和生成模型 
```

---

```diff
- 
+ # Vid2Prompt - AI生成视频提示词分析工具
+ 
+ Vid2Prompt是一个用于分析AI生成视频的工具，能够从视频中提取关键帧，并使用CLIP模型生成可能的提示词，同时推测视频可能使用的AI生成模型。
+ 
+ ## 功能特点
+ 
+ - 自动从视频中等间隔提取3-5帧关键图像
+ - 使用CLIP-Interrogator分析每一帧，生成详细的文本描述
+ - 汇总分析结果，生成综合性的提示词
+ - 根据图像特征和提示词，推测可能使用的生成模型（如Sora、Runway Gen-2等）
+ - 提供友好的Streamlit界面，展示分析结果
+ 
+ ## 安装方法
+ 
+ 1. 克隆仓库：
+ ```bash
+ git clone https://github.com/yourusername/Vid2Prompt.git
+ cd Vid2Prompt
+ ```
+ 
+ 2. 安装依赖：
+ ```bash
+ pip install -r requirements.txt
+ ```
+ 
+ ## 使用方法
+ 
+ 启动Streamlit应用：
+ ```bash
+ streamlit run app.py
+ ```
+ 
+ 然后在浏览器中访问显示的地址（通常是 http://localhost:8501）。
+ 
+ ## 使用流程
+ 
+ 1. 上传AI生成的视频文件（支持mp4、avi、mov、mkv格式）
+ 2. 系统自动分析视频，提取关键帧
+ 3. 使用CLIP-Interrogator生成每一帧的描述
+ 4. 汇总分析结果，生成综合提示词
+ 5. 推测可能使用的AI生成模型
+ 
+ ## 系统要求
+ 
+ - Python 3.8+
+ - 推荐使用GPU以加速CLIP模型处理
+ - 至少4GB内存（使用GPU时推荐8GB+）
+ 
+ ## 技术栈
+ 
+ - Streamlit：用户界面
+ - OpenCV：视频处理
+ - CLIP-Interrogator：图像分析和提示词生成
+ - PyTorch：深度学习框架
+ - NumPy：数据处理
+ 
+ ## 注意事项
+ 
+ - 首次运行时会下载CLIP和BLIP模型，可能需要一些时间
+ - 分析结果仅供参考，不能保证100%准确
+ - 对于复杂视频，推测的提示词可能不完全匹配原始提示词 
```

---

让我们添加一个简单的启动脚本：

---

```diff
- 
+ #!/usr/bin/env python
+ # -*- coding: utf-8 -*-
+ 
+ """
+ Vid2Prompt 启动脚本
+ """
+ 
+ import os
+ import sys
+ import subprocess
+ 
+ def main():
+     """启动Streamlit应用"""
+     print("正在启动Vid2Prompt应用...")
+     
+     # 检查依赖是否已安装
+     try:
+         import streamlit
+         import torch
+         import cv2
+         import numpy
+         from clip_interrogator import Interrogator
+     except ImportError as e:
+         print(f"缺少必要依赖: {e}")
+         print("请先运行: pip install -r requirements.txt")
+         return 1
+     
+     # 检查CUDA是否可用
+     import torch
+     if torch.cuda.is_available():
+         device = "CUDA"
+         device_name = torch.cuda.get_device_name(0)
+         print(f"检测到GPU: {device_name}")
+     else:
+         device = "CPU"
+         print("未检测到GPU，将使用CPU运行（处理速度可能较慢）")
+     
+     print(f"使用设备: {device}")
+     print("正在启动界面...")
+     
+     # 启动Streamlit应用
+     streamlit_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "app.py")
+     subprocess.run([sys.executable, "-m", "streamlit", "run", streamlit_path])
+     
+     return 0
+ 
+ if __name__ == "__main__":
+     sys.exit(main()) 
```

---

让我们查看项目结构：

---

<details>
            <summary>Listed current directory • **7** results</summary>
        
| Name |
|-------|
| 📄 `run.py` |
| 📄 `README.md` |
| 📁 `src` |
| 📄 `app.py` |
| 📄 `requirements.txt` |
| 📁 `.specstory` |
| 📄 `.cursorindexingignore` |

</details>

---

<details>
            <summary>Listed directory src • **4** results</summary>
        
| Name |
|-------|
| 📄 `__init__.py` |
| 📄 `model_detector.py` |
| 📄 `prompt_generator.py` |
| 📄 `video_processor.py` |

</details>

---

现在我们已经完成了项目的基本结构和代码实现。让我们总结一下项目内容：

1. **项目结构**：
   - `app.py` - Streamlit应用主文件
   - `run.py` - 启动脚本
   - `requirements.txt` - 依赖列表
   - `src/` - 源代码目录
     - `video_processor.py` - 视频处理模块，用于提取帧
     - `prompt_generator.py` - 提示词生成模块，使用CLIP-Interrogator分析图像
     - `model_detector.py` - 模型检测模块，推测生成视频的AI模型
     - `__init__.py` - 包初始化文件

2. **主要功能**：
   - 从视频中等间隔提取3-5帧图像
   - 使用CLIP-Interrogator分析每一帧，生成描述
   - 汇总分析结果，生成综合提示词
   - 推测可能使用的AI生成模型
   - 提供Streamlit界面展示结果

3. **使用方法**：
   - 安装依赖：`pip install -r requirements.txt`
   - 运行应用：`python run.py` 或 `streamlit run app.py`
   - 在界面上传视频文件进行分析

4. **技术栈**：
   - Streamlit用于界面
   - OpenCV用于视频处理
   - CLIP-Interrogator用于图像分析
   - PyTorch作为深度学习框架
   - NumPy用于数据处理

您可以通过以下命令启动应用：

```bash
pip install -r requirements.txt
python run.py
```

或者直接使用Streamlit命令：

```bash
streamlit run app.py
```

首次运行时，系统会下载CLIP和BLIP模型，可能需要一些时间。推荐在有GPU的环境中运行，以加快处理速度。

如果您希望进一步改进项目，可以考虑：
1. 添加更多AI模型的特征库
2. 优化帧提取算法，选择更有代表性的帧
3. 增加批处理功能，支持多视频分析
4. 添加结果导出功能
5. 实现更精确的模型检测算法

---

_**User**_

把不必上传的大模型写在git ignore里，我传不上

---

_**Assistant**_

我来帮您创建一个`.gitignore`文件，排除CLIP和其他大模型文件，避免上传到Git仓库：

---

```diff
- 
+ # Python缓存文件
+ __pycache__/
+ *.py[cod]
+ *$py.class
+ *.so
+ .Python
+ env/
+ build/
+ develop-eggs/
+ dist/
+ downloads/
+ eggs/
+ .eggs/
+ lib/
+ lib64/
+ parts/
+ sdist/
+ var/
+ *.egg-info/
+ .installed.cfg
+ *.egg
+ 
+ # 虚拟环境
+ venv/
+ ENV/
+ env/
+ 
+ # CLIP和BLIP模型文件
+ *.pt
+ *.pth
+ *.bin
+ *.onnx
+ *.safetensors
+ 
+ # CLIP-Interrogator缓存目录
+ .cache/
+ .clip_interrogator_cache/
+ clip_interrogator_cache/
+ interrogator_cache/
+ .interrogator/
+ 
+ # 模型下载目录
+ models/
+ pretrained/
+ weights/
+ checkpoints/
+ .torch/
+ .cache/torch/
+ .cache/huggingface/
+ 
+ # 临时文件
+ .temp/
+ tmp/
+ temp/
+ *.tmp
+ 
+ # Jupyter Notebook
+ .ipynb_checkpoints
+ 
+ # Streamlit
+ .streamlit/
+ 
+ # 系统文件
+ .DS_Store
+ Thumbs.db
+ 
+ # 日志文件
+ *.log
+ logs/
+ 
+ # 视频和图像样本（可选，如果不想包含示例文件）
+ samples/
+ examples/
+ *.mp4
+ *.avi
+ *.mov
+ *.mkv
+ *.jpg
+ *.jpeg
+ *.png
+ *.gif 
```

---

我已创建了`.gitignore`文件，主要排除了以下内容：

1. **模型文件**：
   - 所有常见模型格式（.pt、.pth、.bin、.onnx、.safetensors）
   - CLIP-Interrogator缓存目录
   - Hugging Face和PyTorch缓存目录

2. **Python相关文件**：
   - Python缓存文件
   - 虚拟环境目录
   - 构建和分发文件

3. **其他大文件**：
   - 视频和图像文件（.mp4、.avi、.jpg等）
   - 示例和样本目录

这样设置后，当您将代码推送到Git仓库时，这些大文件和模型将被忽略，不会被上传，从而避免仓库体积过大的问题。用户在克隆您的仓库后，会在首次运行时自动下载所需的模型文件。

---

_**User**_

运行程序就会自动下载的不用其它操作对吧

---

_**Assistant**_

是的，运行程序时会自动下载所需的模型，不需要其他额外操作。

当用户首次运行您的程序时：

1. CLIP-Interrogator会自动下载所需的CLIP和BLIP模型
2. 这些模型会被缓存在用户的本地目录中（通常在`.cache`或类似目录）
3. 下载过程是透明的，用户无需手动干预

您的`run.py`脚本中已经包含了依赖检查，如果缺少必要的库会提示用户安装。而模型的下载是由CLIP-Interrogator库在首次使用时自动处理的。

用户只需要执行：
```bash
pip install -r requirements.txt
python run.py
```

或者：
```bash
streamlit run app.py
```

系统会自动完成所有必要的下载和设置。首次运行可能需要较长时间，因为需要下载模型文件，但后续运行会直接使用缓存的模型，速度会快很多。

---

_**User**_

为什么提示词是这样的：araffe on the floor of a subway train with a person laying on the floor, araffe, floor, train, man, subway, laying, washington, inspired, wearing, purple, undershirt, fish, très, détaillé, tiktok, 8 h, megalophoba, midlands, très détaillé, grape

好像不知道是什么语言，为什么会这样？我希望是英语，这样我好去平台复现这个视频生成过程

---

_**Assistant**_

这个提示词混合了英语和法语，并且包含了一些可能不相关的词汇。这种情况出现的原因有几个：

1. **CLIP-Interrogator特性**：CLIP-Interrogator在分析图像时，会尝试匹配其训练数据中的各种标签和描述，有时会产生混合语言或不相关词汇。

2. **法语词汇**：例如"très détaillé"是法语，意思是"非常详细"，这是因为CLIP模型的训练数据中包含多语言内容。

3. **无关词汇**：像"tiktok"、"8 h"、"megalophoba"、"midlands"等可能是模型错误地将图像中的某些元素与这些标签关联。

让我们修改代码，使提示词更加规范化，只保留英语内容：

---



---



---

